{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "강화학습 예제",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNOIwdqQeDMqPTYBn/QYAvc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sjbaek12/sjbaek12.github.io/blob/master/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5_%EC%98%88%EC%A0%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4vqY6BwzMBK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Oo2ymghzhAl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "env = gym.make(\"CartPole-v0\")\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keKQ6vi-zftl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "87100322-6bb1-460d-a223-9868e4d0c3c5"
      },
      "source": [
        "# Try running environment with random actions\n",
        "\n",
        "# CartPole은 막대기를 쓰러뜨리지 않고 옮기는 것인데 매 에피소드마다 최대보상이 200이고, \n",
        "# 매번 step마다 1의 보상을 얻는데 100에피소드를 해서 모두 195점 이상 얻으면 학습이 완료된 것으로 인정한다.\n",
        "\n",
        "env.reset()  # 새로운 에피소드를 시작한다. \n",
        "reward_sum = 0\n",
        "num_games = 10\n",
        "num_game = 0\n",
        "while num_game < num_games:\n",
        "#     env.render()  # 행동을 하기전 환경에 대해 얻은 관찰값을 그린다.\n",
        "    observation, reward, done, _ = env.step(env.action_space.sample())\n",
        "    # env.action_space.sample() 환경에서 행동 샘플링이다. 1은 오른쪽 이동 , 0은 왼쪽 이동이다.\n",
        "    # env.step 행동을 환경으로 내보낸다. 그러면 행동으로 인한 상태, 리워드가 만들어진다.\n",
        "    # observation은 4개의 실수로 구성된 배열로 state를 나타낸다\n",
        "    # done은 episode가 종료되었을때 True 이다. 막대기가 쓰러지면 종료된다.\n",
        "    print(observation, reward, done)\n",
        "    reward_sum += reward\n",
        "    if done:\n",
        "        print(\"Reward for this episode was: {}\".format(reward_sum))\n",
        "        reward_sum = 0\n",
        "        num_game += 1\n",
        "        env.reset()\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.01682308  0.18159763 -0.04820731 -0.26364786] 1.0 False\n",
            "[-0.01319113 -0.01280426 -0.05348026  0.01344876] 1.0 False\n",
            "[-0.01344721 -0.20712007 -0.05321129  0.28879004] 1.0 False\n",
            "[-0.01758962 -0.40144444 -0.04743549  0.56422729] 1.0 False\n",
            "[-0.0256185  -0.20569015 -0.03615094  0.25698534] 1.0 False\n",
            "[-0.02973231 -0.01007122 -0.03101123 -0.0468774 ] 1.0 False\n",
            "[-0.02993373 -0.20473508 -0.03194878  0.23586216] 1.0 False\n",
            "[-0.03402843 -0.39938635 -0.02723154  0.51829872] 1.0 False\n",
            "[-0.04201616 -0.20389179 -0.01686556  0.21716053] 1.0 False\n",
            "[-0.046094   -0.39876863 -0.01252235  0.50447597] 1.0 False\n",
            "[-0.05406937 -0.20347246 -0.00243283  0.20787321] 1.0 False\n",
            "[-0.05813882 -0.39855954  0.00172463  0.49978771] 1.0 False\n",
            "[-0.06611001 -0.20346195  0.01172038  0.20764879] 1.0 False\n",
            "[-0.07017925 -0.39874951  0.01587336  0.50400574] 1.0 False\n",
            "[-0.07815424 -0.59409154  0.02595347  0.80164843] 1.0 False\n",
            "[-0.09003607 -0.39933496  0.04198644  0.51724141] 1.0 False\n",
            "[-0.09802277 -0.20482856  0.05233127  0.23807934] 1.0 False\n",
            "[-0.10211934 -0.01049176  0.05709286 -0.03764843] 1.0 False\n",
            "[-0.10232917 -0.20638398  0.05633989  0.27248731] 1.0 False\n",
            "[-0.10645685 -0.01210931  0.06178964 -0.00190715] 1.0 False\n",
            "[-0.10669904 -0.20806049  0.06175149  0.30961329] 1.0 False\n",
            "[-0.11086025 -0.01387024  0.06794376  0.03702652] 1.0 False\n",
            "[-0.11113766 -0.20989741  0.06868429  0.35034892] 1.0 False\n",
            "[-0.1153356  -0.01581606  0.07569127  0.08009064] 1.0 False\n",
            "[-0.11565192 -0.21193687  0.07729308  0.39566153] 1.0 False\n",
            "[-0.11989066 -0.40806555  0.08520631  0.71167774] 1.0 False\n",
            "[-0.12805197 -0.60425755  0.09943987  1.02991847] 1.0 False\n",
            "[-0.14013712 -0.80055194  0.12003824  1.35209265] 1.0 False\n",
            "[-0.15614816 -0.6071244   0.14708009  1.09924573] 1.0 False\n",
            "[-0.16829065 -0.4142119   0.169065    0.85608704] 1.0 False\n",
            "[-0.17657489 -0.22174705  0.18618674  0.62097452] 1.0 False\n",
            "[-0.18100983 -0.4189139   0.19860623  0.96603663] 1.0 False\n",
            "[-0.18938811 -0.22693315  0.21792697  0.74173341] 1.0 True\n",
            "Reward for this episode was: 33.0\n",
            "[-0.00758242 -0.21919668 -0.05009391  0.25151924] 1.0 False\n",
            "[-0.01196635 -0.02339652 -0.04506353 -0.05653402] 1.0 False\n",
            "[-0.01243428  0.17234163 -0.04619421 -0.36308736] 1.0 False\n",
            "[-0.00898745  0.36808865 -0.05345596 -0.66997094] 1.0 False\n",
            "[-0.00162568  0.17374907 -0.06685537 -0.39458654] 1.0 False\n",
            "[ 0.0018493   0.36975282 -0.07474711 -0.7075762 ] 1.0 False\n",
            "[ 0.00924436  0.17574155 -0.08889863 -0.43932709] 1.0 False\n",
            "[ 0.01275919 -0.01801714 -0.09768517 -0.17593853] 1.0 False\n",
            "[ 0.01239885 -0.21161524 -0.10120394  0.08440039] 1.0 False\n",
            "[ 0.00816654 -0.40515182 -0.09951593  0.34351665] 1.0 False\n",
            "[ 6.35083520e-05 -5.98727599e-01 -9.26456011e-02  6.03234293e-01] 1.0 False\n",
            "[-0.01191104 -0.40244033 -0.08058092  0.28286721] 1.0 False\n",
            "[-0.01995985 -0.59632599 -0.07492357  0.54908684] 1.0 False\n",
            "[-0.03188637 -0.40023596 -0.06394183  0.23377004] 1.0 False\n",
            "[-0.03989109 -0.20426144 -0.05926643 -0.07837709] 1.0 False\n",
            "[-0.04397632 -0.00834216 -0.06083398 -0.38915417] 1.0 False\n",
            "[-0.04414316 -0.20255027 -0.06861706 -0.11625503] 1.0 False\n",
            "[-0.04819417 -0.39662539 -0.07094216  0.154015  ] 1.0 False\n",
            "[-0.05612667 -0.20056315 -0.06786186 -0.16017812] 1.0 False\n",
            "[-0.06013794 -0.39465125 -0.07106542  0.11034784] 1.0 False\n",
            "[-0.06803096 -0.5886867  -0.06885846  0.37979073] 1.0 False\n",
            "[-0.0798047  -0.39265793 -0.06126265  0.066216  ] 1.0 False\n",
            "[-0.08765786 -0.19671358 -0.05993833 -0.2451491 ] 1.0 False\n",
            "[-0.09159213 -0.00078904 -0.06484131 -0.55611948] 1.0 False\n",
            "[-0.09160791  0.19518048 -0.0759637  -0.86850601] 1.0 False\n",
            "[-0.0877043   0.3912492  -0.09333382 -1.18407279] 1.0 False\n",
            "[-0.07987931  0.19745372 -0.11701528 -0.92204512] 1.0 False\n",
            "[-0.07593024  0.00409067 -0.13545618 -0.66830805] 1.0 False\n",
            "[-0.07584843  0.20081025 -0.14882234 -1.00038688] 1.0 False\n",
            "[-0.07183222  0.00795672 -0.16883008 -0.75789662] 1.0 False\n",
            "[-0.07167309  0.20495305 -0.18398801 -1.09859037] 1.0 False\n",
            "[-0.06757403  0.01266613 -0.20595982 -0.86881594] 1.0 False\n",
            "[-0.0673207  -0.17914849 -0.22333614 -0.64729997] 1.0 True\n",
            "Reward for this episode was: 33.0\n",
            "[ 0.00306488  0.24478585 -0.03514204 -0.310318  ] 1.0 False\n",
            "[ 0.00796059  0.05018175 -0.0413484  -0.02892171] 1.0 False\n",
            "[ 0.00896423 -0.1443236  -0.04192684  0.25043397] 1.0 False\n",
            "[ 0.00607776 -0.33882253 -0.03691816  0.52960309] 1.0 False\n",
            "[-6.98693339e-04 -5.33406200e-01 -2.63260974e-02  8.10428243e-01] 1.0 False\n",
            "[-0.01136682 -0.33793365 -0.01011753  0.50958203] 1.0 False\n",
            "[-1.81254903e-02 -1.42670630e-01  7.41081322e-05  2.13727997e-01] 1.0 False\n",
            "[-0.0209789   0.05245026  0.00434867 -0.07893155] 1.0 False\n",
            "[-0.0199299  -0.14273376  0.00277004  0.21512021] 1.0 False\n",
            "[-0.02278457 -0.3378952   0.00707244  0.50867565] 1.0 False\n",
            "[-0.02954248 -0.53311608  0.01724595  0.80357892] 1.0 False\n",
            "[-0.0402048  -0.7284702   0.03331753  1.10163658] 1.0 False\n",
            "[-0.0547742  -0.9240143   0.05535026  1.40458349] 1.0 False\n",
            "[-0.07325449 -0.72962158  0.08344193  1.12970564] 1.0 False\n",
            "[-0.08784692 -0.53568556  0.10603605  0.86431721] 1.0 False\n",
            "[-0.09856063 -0.34215444  0.12332239  0.6067672 ] 1.0 False\n",
            "[-0.10540372 -0.14895296  0.13545774  0.35533086] 1.0 False\n",
            "[-0.10838278  0.04400915  0.14256435  0.108242  ] 1.0 False\n",
            "[-0.1075026  -0.1528374   0.14472919  0.44228837] 1.0 False\n",
            "[-0.11055934 -0.349679    0.15357496  0.77686578] 1.0 False\n",
            "[-0.11755292 -0.15696497  0.16911228  0.53617072] 1.0 False\n",
            "[-0.12069222  0.03542607  0.17983569  0.30118333] 1.0 False\n",
            "[-0.1199837  -0.16174234  0.18585936  0.64475354] 1.0 False\n",
            "[-0.12321855 -0.35890125  0.19875443  0.98972725] 1.0 False\n",
            "[-0.13039657 -0.16691422  0.21854897  0.76546594] 1.0 True\n",
            "Reward for this episode was: 25.0\n",
            "[-0.00770934  0.23407758 -0.03741837 -0.31270258] 1.0 False\n",
            "[-0.00302779  0.42971206 -0.04367242 -0.61694732] 1.0 False\n",
            "[ 0.00556645  0.23522655 -0.05601137 -0.33833283] 1.0 False\n",
            "[ 0.01027098  0.0409445  -0.06277802 -0.06382551] 1.0 False\n",
            "[ 0.01108987 -0.15322381 -0.06405453  0.20840861] 1.0 False\n",
            "[ 0.0080254   0.0427528  -0.05988636 -0.1037726 ] 1.0 False\n",
            "[ 0.00888045  0.23867957 -0.06196181 -0.41473199] 1.0 False\n",
            "[ 0.01365404  0.43462251 -0.07025645 -0.72628748] 1.0 False\n",
            "[ 0.02234649  0.63064192 -0.0847822  -1.04022965] 1.0 False\n",
            "[ 0.03495933  0.82678165 -0.10558679 -1.35827859] 1.0 False\n",
            "[ 0.05149496  0.63313042 -0.13275237 -1.10040433] 1.0 False\n",
            "[ 0.06415757  0.82972556 -0.15476045 -1.43161703] 1.0 False\n",
            "[ 0.08075208  0.636815   -0.18339279 -1.19102771] 1.0 False\n",
            "[ 0.09348838  0.44448025 -0.20721335 -0.96097934] 1.0 False\n",
            "[ 0.10237799  0.64169347 -0.22643293 -1.3109567 ] 1.0 True\n",
            "Reward for this episode was: 15.0\n",
            "[-0.03645142 -0.23133522 -0.03815107  0.32684064] 1.0 False\n",
            "[-0.04107812 -0.03569146 -0.03161426  0.02237485] 1.0 False\n",
            "[-0.04179195 -0.23034611 -0.03116676  0.30491787] 1.0 False\n",
            "[-0.04639887 -0.03479419 -0.0250684   0.0025709 ] 1.0 False\n",
            "[-0.04709476  0.16067814 -0.02501698 -0.29791481] 1.0 False\n",
            "[-0.04388119  0.35614761 -0.03097528 -0.59838149] 1.0 False\n",
            "[-0.03675824  0.55168896 -0.04294291 -0.9006581 ] 1.0 False\n",
            "[-0.02572446  0.74736566 -0.06095607 -1.20652389] 1.0 False\n",
            "[-0.01077715  0.55308199 -0.08508655 -0.93354961] 1.0 False\n",
            "[ 2.84491891e-04  7.49242401e-01 -1.03757543e-01 -1.25171104e+00] 1.0 False\n",
            "[ 0.01526934  0.55559151 -0.12879176 -0.99324689] 1.0 False\n",
            "[ 0.02638117  0.36240582 -0.1486567  -0.74362855] 1.0 False\n",
            "[ 0.03362929  0.55923263 -0.16352927 -1.07915681] 1.0 False\n",
            "[ 0.04481394  0.36660294 -0.18511241 -0.84193074] 1.0 False\n",
            "[ 0.052146    0.56370358 -0.20195102 -1.18664344] 1.0 False\n",
            "[ 0.06342007  0.76078795 -0.22568389 -1.53523285] 1.0 True\n",
            "Reward for this episode was: 16.0\n",
            "[-0.02281698  0.16895873 -0.01859003 -0.25930984] 1.0 False\n",
            "[-0.01943781 -0.02589297 -0.02377623  0.02745205] 1.0 False\n",
            "[-0.01995567 -0.22066602 -0.02322719  0.3125395 ] 1.0 False\n",
            "[-0.02436899 -0.41544951 -0.0169764   0.59780774] 1.0 False\n",
            "[-0.03267798 -0.22009417 -0.00502024  0.29982614] 1.0 False\n",
            "[-0.03707986 -0.02490103  0.00097628  0.00556417] 1.0 False\n",
            "[-0.03757788  0.17020691  0.00108756 -0.28681057] 1.0 False\n",
            "[-0.03417375  0.36531333 -0.00464865 -0.57915029] 1.0 False\n",
            "[-0.02686748  0.17025684 -0.01623165 -0.28793541] 1.0 False\n",
            "[-0.02346234  0.36560646 -0.02199036 -0.58569313] 1.0 False\n",
            "[-0.01615021  0.17079931 -0.03370423 -0.30001771] 1.0 False\n",
            "[-0.01273423  0.36638505 -0.03970458 -0.60313687] 1.0 False\n",
            "[-0.00540653  0.56203918 -0.05176732 -0.90805684] 1.0 False\n",
            "[ 0.00583426  0.36765478 -0.06992845 -0.63208364] 1.0 False\n",
            "[ 0.01318735  0.17357454 -0.08257013 -0.36221698] 1.0 False\n",
            "[ 0.01665884 -0.02028268 -0.08981447 -0.0966699 ] 1.0 False\n",
            "[ 0.01625319  0.17600406 -0.09174786 -0.41628325] 1.0 False\n",
            "[ 0.01977327 -0.01770598 -0.10007353 -0.15387632] 1.0 False\n",
            "[ 0.01941915  0.17869586 -0.10315106 -0.47637767] 1.0 False\n",
            "[ 0.02299307 -0.01482988 -0.11267861 -0.21790469] 1.0 False\n",
            "[ 0.02269647 -0.20817585 -0.1170367   0.03721764] 1.0 False\n",
            "[ 0.01853296 -0.40144194 -0.11629235  0.2908033 ] 1.0 False\n",
            "[ 0.01050412 -0.59473025 -0.11047628  0.54466452] 1.0 False\n",
            "[-0.00139049 -0.39824344 -0.09958299  0.21931707] 1.0 False\n",
            "[-0.00935536 -0.59181132 -0.09519665  0.47900137] 1.0 False\n",
            "[-0.02119158 -0.39548342 -0.08561662  0.15789734] 1.0 False\n",
            "[-0.02910125 -0.58928188 -0.08245868  0.4223897 ] 1.0 False\n",
            "[-0.04088689 -0.78314471 -0.07401088  0.6879805 ] 1.0 False\n",
            "[-0.05654978 -0.58707779 -0.06025127  0.3729459 ] 1.0 False\n",
            "[-0.06829134 -0.78129439 -0.05279236  0.64603901] 1.0 False\n",
            "[-0.08391723 -0.5854781  -0.03987158  0.33721005] 1.0 False\n",
            "[-0.09562679 -0.38981211 -0.03312737  0.03222519] 1.0 False\n",
            "[-0.10342303 -0.19423114 -0.03248287 -0.27072296] 1.0 False\n",
            "[-0.10730765 -0.38887487 -0.03789733  0.01154032] 1.0 False\n",
            "[-0.11508515 -0.58343341 -0.03766652  0.29202944] 1.0 False\n",
            "[-0.12675382 -0.38779521 -0.03182593 -0.01229105] 1.0 False\n",
            "[-0.13450972 -0.58244662 -0.03207176  0.27018296] 1.0 False\n",
            "[-0.14615866 -0.38688203 -0.0266681  -0.03244055] 1.0 False\n",
            "[-0.1538963  -0.5816116  -0.02731691  0.25171045] 1.0 False\n",
            "[-0.16552853 -0.38611044 -0.0222827  -0.04946206] 1.0 False\n",
            "[-0.17325074 -0.19067618 -0.02327194 -0.34909124] 1.0 False\n",
            "[-0.17706426  0.0047689  -0.03025376 -0.6490208 ] 1.0 False\n",
            "[-0.17696888  0.20029895 -0.04323418 -0.95107491] 1.0 False\n",
            "[-0.1729629   0.39597528 -0.06225568 -1.25702202] 1.0 False\n",
            "[-0.1650434   0.20170297 -0.08739612 -0.98446945] 1.0 False\n",
            "[-0.16100934  0.00785357 -0.10708551 -0.72046688] 1.0 False\n",
            "[-0.16085227  0.20428126 -0.12149485 -1.04484365] 1.0 False\n",
            "[-0.15676664  0.01096331 -0.14239172 -0.79263595] 1.0 False\n",
            "[-0.15654738 -0.18194674 -0.15824444 -0.54792126] 1.0 False\n",
            "[-0.16018631 -0.37453315 -0.16920286 -0.308981  ] 1.0 False\n",
            "[-0.16767697 -0.17745524 -0.17538248 -0.64988487] 1.0 False\n",
            "[-0.17122608 -0.36975672 -0.18838018 -0.41715606] 1.0 False\n",
            "[-0.17862121 -0.56177954 -0.1967233  -0.18927427] 1.0 False\n",
            "[-0.1898568  -0.36446691 -0.20050879 -0.53699898] 1.0 False\n",
            "[-0.19714614 -0.556289   -0.21124877 -0.31358586] 1.0 True\n",
            "Reward for this episode was: 55.0\n",
            "[-0.02857963  0.20275161  0.00723794 -0.27528389] 1.0 False\n",
            "[-0.0245246   0.00752715  0.00173226  0.01967307] 1.0 False\n",
            "[-0.02437405 -0.1876196   0.00212572  0.31290204] 1.0 False\n",
            "[-0.02812645  0.007472    0.00838376  0.02089026] 1.0 False\n",
            "[-0.02797701 -0.18776917  0.00880157  0.31620653] 1.0 False\n",
            "[-0.03173239 -0.38301538  0.0151257   0.61165212] 1.0 False\n",
            "[-0.0393927  -0.57834543  0.02735874  0.90906047] 1.0 False\n",
            "[-0.05095961 -0.77382679  0.04553995  1.21021525] 1.0 False\n",
            "[-0.06643614 -0.57932149  0.06974426  0.9321439 ] 1.0 False\n",
            "[-0.07802257 -0.77531172  0.08838713  1.24590271] 1.0 False\n",
            "[-0.09352881 -0.97144918  0.11330519  1.56491243] 1.0 False\n",
            "[-0.11295779 -0.77784927  0.14460344  1.30961515] 1.0 False\n",
            "[-0.12851478 -0.58482451  0.17079574  1.06546529] 1.0 False\n",
            "[-0.14021127 -0.39232333  0.19210505  0.83088507] 1.0 False\n",
            "[-0.14805773 -0.20027282  0.20872275  0.60424074] 1.0 False\n",
            "[-0.15206319 -0.00858648  0.22080756  0.38387156] 1.0 True\n",
            "Reward for this episode was: 16.0\n",
            "[ 0.01064226  0.20018412 -0.01776752 -0.33507843] 1.0 False\n",
            "[ 0.01464595  0.39555437 -0.02446909 -0.63331093] 1.0 False\n",
            "[ 0.02255703  0.59100897 -0.03713531 -0.93359825] 1.0 False\n",
            "[ 0.03437721  0.39640716 -0.05580727 -0.65281206] 1.0 False\n",
            "[ 0.04230536  0.59226003 -0.06886351 -0.96253283] 1.0 False\n",
            "[ 0.05415056  0.78823638 -0.08811417 -1.27603048] 1.0 False\n",
            "[ 0.06991528  0.59434166 -0.11363478 -1.01218879] 1.0 False\n",
            "[ 0.08180212  0.40090405 -0.13387855 -0.75724051] 1.0 False\n",
            "[ 0.0898202   0.59759207 -0.14902336 -1.08887531] 1.0 False\n",
            "[ 0.10177204  0.79433065 -0.17080087 -1.42436325] 1.0 False\n",
            "[ 0.11765865  0.99110239 -0.19928814 -1.76519766] 1.0 False\n",
            "[ 0.1374807   0.79871335 -0.23459209 -1.54052947] 1.0 True\n",
            "Reward for this episode was: 12.0\n",
            "[ 0.02289349 -0.15115994 -0.02870694  0.29508067] 1.0 False\n",
            "[ 0.0198703  -0.34586112 -0.02280532  0.57857343] 1.0 False\n",
            "[ 0.01295307 -0.15042711 -0.01123385  0.27879445] 1.0 False\n",
            "[ 0.00994453  0.04485329 -0.00565796 -0.01741034] 1.0 False\n",
            "[ 0.0108416  -0.15018707 -0.00600617  0.27348207] 1.0 False\n",
            "[ 0.00783786  0.04502007 -0.00053653 -0.02108915] 1.0 False\n",
            "[ 0.00873826  0.24014971 -0.00095831 -0.31394131] 1.0 False\n",
            "[ 0.01354125  0.04504142 -0.00723714 -0.02156076] 1.0 False\n",
            "[ 0.01444208  0.24026641 -0.00766835 -0.31651827] 1.0 False\n",
            "[ 0.01924741  0.04525452 -0.01399872 -0.02626351] 1.0 False\n",
            "[ 0.0201525   0.24057439 -0.01452399 -0.32333011] 1.0 False\n",
            "[ 0.02496399  0.43590012 -0.02099059 -0.62055769] 1.0 False\n",
            "[ 0.03368199  0.63130883 -0.03340175 -0.91977697] 1.0 False\n",
            "[ 0.04630816  0.82686593 -0.05179729 -1.22276729] 1.0 False\n",
            "[ 0.06284548  0.63244811 -0.07625263 -0.94675328] 1.0 False\n",
            "[ 0.07549445  0.43843127 -0.0951877  -0.67897024] 1.0 False\n",
            "[ 0.08426307  0.63473762 -0.1087671  -1.00003971] 1.0 False\n",
            "[ 0.09695782  0.44122421 -0.1287679  -0.74339941] 1.0 False\n",
            "[ 0.10578231  0.24809243 -0.14363588 -0.4938534 ] 1.0 False\n",
            "[ 0.11074416  0.05525726 -0.15351295 -0.24966324] 1.0 False\n",
            "[ 0.1118493  -0.13737725 -0.15850622 -0.0090653 ] 1.0 False\n",
            "[ 0.10910176 -0.32991289 -0.15868752  0.22971184] 1.0 False\n",
            "[ 0.1025035  -0.52245338 -0.15409329  0.46843524] 1.0 False\n",
            "[ 0.09205443 -0.71510101 -0.14472458  0.70885835] 1.0 False\n",
            "[ 0.07775241 -0.90795346 -0.13054741  0.95271215] 1.0 False\n",
            "[ 0.05959334 -1.10110015 -0.11149317  1.20169489] 1.0 False\n",
            "[ 0.03757134 -1.29461781 -0.08745927  1.45745794] 1.0 False\n",
            "[ 0.01167898 -1.09853843 -0.05831011  1.13878278] 1.0 False\n",
            "[-0.01029179 -0.90270452 -0.03553446  0.82839769] 1.0 False\n",
            "[-0.02834588 -0.70711523 -0.01896651  0.52475404] 1.0 False\n",
            "[-0.04248818 -0.51173157 -0.00847142  0.22615545] 1.0 False\n",
            "[-0.05272281 -0.70673144 -0.00394832  0.51615418] 1.0 False\n",
            "[-0.06685744 -0.90179757  0.00637477  0.80759029] 1.0 False\n",
            "[-0.08489339 -1.0970063   0.02252657  1.10227161] 1.0 False\n",
            "[-0.10683352 -0.90218784  0.04457201  0.81674031] 1.0 False\n",
            "[-0.12487728 -0.70770353  0.06090681  0.5384034 ] 1.0 False\n",
            "[-0.13903135 -0.51348835  0.07167488  0.26551635] 1.0 False\n",
            "[-0.14930111 -0.31945869  0.07698521 -0.00372649] 1.0 False\n",
            "[-0.15569029 -0.1255204   0.07691068 -0.27116099] 1.0 False\n",
            "[-0.1582007   0.06842462  0.07148746 -0.53862906] 1.0 False\n",
            "[-0.1568322   0.26247261  0.06071488 -0.80795818] 1.0 False\n",
            "[-0.15158275  0.45671227  0.04455571 -1.08094162] 1.0 False\n",
            "[-0.14244851  0.26103135  0.02293688 -0.7746165 ] 1.0 False\n",
            "[-0.13722788  0.4558304   0.00744455 -1.05999535] 1.0 False\n",
            "[-0.12811127  0.65085296 -0.01375536 -1.35033241] 1.0 False\n",
            "[-0.11509421  0.45590648 -0.040762   -1.06198431] 1.0 False\n",
            "[-0.10597608  0.65154376 -0.06200169 -1.36717718] 1.0 False\n",
            "[-0.09294521  0.84738462 -0.08934523 -1.67859083] 1.0 False\n",
            "[-0.07599751  0.65340493 -0.12291705 -1.41501442] 1.0 False\n",
            "[-0.06292942  0.46000131 -0.15121734 -1.16314447] 1.0 False\n",
            "[-0.05372939  0.26713654 -0.17448023 -0.92143731] 1.0 False\n",
            "[-0.04838666  0.46413214 -0.19290897 -1.2634815 ] 1.0 False\n",
            "[-0.03910402  0.66112419 -0.2181786  -1.60985261] 1.0 True\n",
            "Reward for this episode was: 53.0\n",
            "[-0.0269759   0.17979112 -0.04457612 -0.31540089] 1.0 False\n",
            "[-0.02338008 -0.01466846 -0.05088413 -0.03710231] 1.0 False\n",
            "[-0.02367344  0.18114485 -0.05162618 -0.34539559] 1.0 False\n",
            "[-0.02005055  0.37696173 -0.05853409 -0.65390091] 1.0 False\n",
            "[-0.01251131  0.18270156 -0.07161211 -0.38020891] 1.0 False\n",
            "[-0.00885728  0.37876349 -0.07921629 -0.694584  ] 1.0 False\n",
            "[-0.00128201  0.18482449 -0.09310797 -0.42785301] 1.0 False\n",
            "[ 0.00241448  0.38113327 -0.10166503 -0.74837495] 1.0 False\n",
            "[ 0.01003714  0.57749977 -0.11663253 -1.07124187] 1.0 False\n",
            "[ 0.02158714  0.77395434 -0.13805737 -1.39813396] 1.0 False\n",
            "[ 0.03706623  0.97049635 -0.16602004 -1.73060195] 1.0 False\n",
            "[ 0.05647615  0.77761443 -0.20063208 -1.49384315] 1.0 False\n",
            "[ 0.07202844  0.97453101 -0.23050895 -1.84188394] 1.0 True\n",
            "Reward for this episode was: 13.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wKo0N2Fzc6x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7317018e-3588-4284-c105-e679e8ff39f8"
      },
      "source": [
        "# 이과정은 성과가 좋은 행동을 mimicking하는 것이다. 상태가 주어지면 action을 output 한다.\n",
        "\n",
        "import keras.layers as layers\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "import keras.backend as K  # 케라스의 backend를 K. 형식으로 호출하는 것이고 여기서는 tensorflow이다.\n",
        "from keras.initializers import glorot_uniform\n",
        "\n",
        "def get_policy_model(env, hidden_layer_neurons, lr):\n",
        "    dimen = env.reset().shape # 환경의 shape이고 이것은 4개의 배열로 구성된다.\n",
        "    num_actions = env.action_space.n  # 가능한 액션의 숫자이다. 여기서는 2로 오른쪽이나 왼쪽으로 움직이는 것\n",
        "    \n",
        "    adv = layers.Input(shape=[1], name=\"advantages\")\n",
        "\n",
        "    # 네개 배열로 구성된 환경이 망으로 들어가서 2개의 가능한 액션을 산출하는 신경망이다.\n",
        "    inp = layers.Input(shape=dimen,name=\"input_x\")\n",
        "    x = layers.Dense(hidden_layer_neurons, \n",
        "                     activation=\"relu\", \n",
        "                     use_bias=False,\n",
        "                     kernel_initializer=glorot_uniform(seed=42),\n",
        "                     name=\"dense_1\")(inp)\n",
        "    out = layers.Dense(num_actions,   # num_actions = 2 이므로 두개의 unit이 설정된다.\n",
        "                       activation=\"softmax\", \n",
        "                       kernel_initializer=glorot_uniform(seed=42),\n",
        "                       use_bias=False,\n",
        "                       name=\"out\")(x)\n",
        "\n",
        "    def custom_loss(y_true, y_pred):\n",
        "        # actual: 0 predict: 0 -> log(0 * (0 - 0) + (1 - 0) * (0 + 0)) = -inf\n",
        "        # actual: 1 predict: 1 -> log(1 * (1 - 1) + (1 - 1) * (1 + 1)) = -inf\n",
        "        # actual: 1 predict: 0 -> log(1 * (1 - 0) + (1 - 1) * (1 + 0)) = 0\n",
        "        # actual: 0 predict: 1 -> log(0 * (0 - 1) + (1 - 0) * (0 + 1)) = 0\n",
        "        log_lik = K.log(y_true * (y_true - y_pred) + (1 - y_true) * (y_true + y_pred))\n",
        "        return K.mean(log_lik * adv, keepdims=True)\n",
        "        \n",
        "    model_train = Model(inputs=[inp, adv], outputs=out)  # state가 input되고 action이 output이다. \n",
        "    model_train.compile(loss=custom_loss, optimizer=Adam(lr)) # 그러나 손실함수가 커스텀이다. output이 두개이므로 미분 가능한 형태로 하나로 합쳐서 만들어낸 것이다.\n",
        "    model_predict = Model(inputs=[inp], outputs=out)\n",
        "    return model_train, model_predict  # 이 함수의 산출물은 모델 자체와 예측값이다."
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFtxrB_Mzksd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discount_rewards(r, gamma=0.99):\n",
        "    \"\"\"Takes 1d float array of rewards and computes discounted reward\n",
        "    e.g. f([1, 1, 1], 0.99) -> [2.9701, 1.99, 1]\n",
        "    \"\"\"\n",
        "    prior = 0\n",
        "    out = []\n",
        "    for val in r:\n",
        "        new_val = val + prior * gamma\n",
        "        out.append(new_val)\n",
        "        prior = new_val\n",
        "    return np.array(out[::-1])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KiANV6ozlet",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Constants defining our neural network\n",
        "hidden_layer_neurons = 8\n",
        "gamma = .99\n",
        "dimen = len(env.reset())  # dimen = 4 이다.\n",
        "print_every = 100\n",
        "batch_size = 50\n",
        "num_episodes = 10000\n",
        "render = False\n",
        "lr = 1e-2\n",
        "goal = 100  # 않쓰러지는 목표 점수 같음"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1q3gEZTCzoON",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# See our trained bot in action, 테스트 단계이다. 기존에 랜덤하게 만들어진 상황에 최상의 성과를 가져오는 액션을 산출하는 \n",
        "# 정책신경망에 random하게 만들어진 새로운 환경을 주입하고 그 것이 어떤 성과를 가져오는지 계산하는 것이다.\n",
        "def score_model(model, num_tests, render=False):\n",
        "    scores = []    \n",
        "    for num_test in range(num_tests):\n",
        "        observation = env.reset()\n",
        "        reward_sum = 0\n",
        "        while True:\n",
        "            if render:\n",
        "                env.render() # 행동을 하기전 환경에 대해 얻은 관찰값을 그린다\n",
        "\n",
        "            state = np.reshape(observation, [1, dimen]) # observation은 (4,) --> state는 (1,4)\n",
        "            predict = model.predict([state])[0] # 그 상태에서 행동을 다시 예측한다.\n",
        "            action = np.argmax(predict) # 예측값중에서 몇번째가 최대값인지 알아낸다. 즉 random하게 한 행동 중에서 어떤 것이 최대의 성과를 냇는지 알아내는 것이다.\n",
        "            observation, reward, done, _ = env.step(action)\n",
        "            reward_sum += reward\n",
        "            if done:\n",
        "                break\n",
        "        scores.append(reward_sum)\n",
        "    env.close()\n",
        "    return np.mean(scores)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHjhTC5Yzr4F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "e9954a93-90c7-4d59-d270-25da3e82b947"
      },
      "source": [
        "model_train, model_predict = get_policy_model(env, hidden_layer_neurons, lr)\n",
        "model_predict.summary()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_x (InputLayer)         (None, 4)                 0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "out (Dense)                  (None, 2)                 16        \n",
            "=================================================================\n",
            "Total params: 48\n",
            "Trainable params: 48\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z12SgHqpzuX2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "45a4a7b7-ea1e-4e27-b83d-f4b66834f9e0"
      },
      "source": [
        "reward_sum = 0\n",
        "\n",
        "num_actions = env.action_space.n  # num_actions = 2 이다\n",
        "\n",
        "# Placeholders for our observations, outputs and rewards\n",
        "states = np.empty(0).reshape(0,dimen)\n",
        "actions = np.empty(0).reshape(0,1)\n",
        "rewards = np.empty(0).reshape(0,1)\n",
        "discounted_rewards = np.empty(0).reshape(0,1)\n",
        "\n",
        "# Setting up our environment\n",
        "observation = env.reset() # 한개의 state를 발생시킨다.\n",
        "\n",
        "num_episode = 0\n",
        "\n",
        "losses = []\n",
        "\n",
        "while num_episode < num_episodes:\n",
        "    # Append the observations to our batch\n",
        "    state = np.reshape(observation, [1, dimen]) # 하나의 state를 (1, 4) array로 만든다.\n",
        "    \n",
        "    predict = model_predict.predict([state])[0]  # 한개의 state를 inout해서 신경망을 통해 action을 만들어낸다.\n",
        "    action = np.random.choice(range(num_actions),p=predict) # 대안 중에서 하나를 선택한다. p=[0.4, 0.6] 대안이 선택될 확율이다. action은 0 또는 1 이된다.\n",
        "    \n",
        "    # Append the observations and outputs for learning\n",
        "    states = np.vstack([states, state]) # 미리 만들어 놓은 placeholder인 states에 while문 밖에서 만들어낸 state를 append 시키다.\n",
        "    actions = np.vstack([actions, action]) # 동일하게 state가 입력이 되어서 신경망이 산출한 행동을 확율적으로 선택한 행동을 actions에 append한다.\n",
        "    \n",
        "    # Determine the oucome of our action\n",
        "    observation, reward, done, _ = env.step(action) # 신경망에서 산출한 행동이 가져오는 상태, 리워드 등을 체크한다.\n",
        "    reward_sum += reward\n",
        "    rewards = np.vstack([rewards, reward])\n",
        "    \n",
        "    if done:  # 완료가 되고난 후, discount reword를 다시 계산한다.\n",
        "        # Determine standardized rewards\n",
        "        discounted_rewards_episode = discount_rewards(rewards, gamma)       \n",
        "        discounted_rewards = np.vstack([discounted_rewards, discounted_rewards_episode])\n",
        "        \n",
        "        rewards = np.empty(0).reshape(0,1)\n",
        "\n",
        "        if (num_episode + 1) % batch_size == 0:  # batch size = 50\n",
        "            discounted_rewards -= discounted_rewards.mean()\n",
        "            discounted_rewards /= discounted_rewards.std()\n",
        "            discounted_rewards = discounted_rewards.squeeze()\n",
        "            actions = actions.squeeze().astype(int)  # squeeze 함수는 배열에서 차원이 1인 것을 찾아서 없애버린다. 예) (2,2,1) --> (2,2)\n",
        "           \n",
        "            actions_train = np.zeros([len(actions), num_actions])\n",
        "            actions_train[np.arange(len(actions)), actions] = 1\n",
        "            \n",
        "            loss = model_train.train_on_batch([states, discounted_rewards], actions_train)  #####\n",
        "            losses.append(loss)\n",
        "\n",
        "            # Clear out game variables\n",
        "            states = np.empty(0).reshape(0,dimen)\n",
        "            actions = np.empty(0).reshape(0,1)\n",
        "            discounted_rewards = np.empty(0).reshape(0,1)\n",
        "\n",
        "\n",
        "        # Print periodically\n",
        "        if (num_episode + 1) % print_every == 0:\n",
        "            # Print status\n",
        "            score = score_model(model_predict,10)\n",
        "            print(\"Average reward for training episode {}: {:0.2f} Test Score: {:0.2f} Loss: {:0.6f} \".format(\n",
        "                (num_episode + 1), reward_sum/print_every, \n",
        "                score,\n",
        "                np.mean(losses[-print_every:])))\n",
        "            \n",
        "            if score >= goal:\n",
        "                print(\"Solved in {} episodes!\".format(num_episode))\n",
        "                break\n",
        "            reward_sum = 0\n",
        "                \n",
        "        num_episode += 1\n",
        "        observation = env.reset()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average reward for training episode 100: 15.71 Test Score: 9.60 Loss: 0.632057 \n",
            "Average reward for training episode 200: 16.38 Test Score: 11.30 Loss: 0.630221 \n",
            "Average reward for training episode 300: 16.87 Test Score: 11.50 Loss: 0.567501 \n",
            "Average reward for training episode 400: 18.33 Test Score: 15.30 Loss: 0.517211 \n",
            "Average reward for training episode 500: 17.83 Test Score: 10.10 Loss: 0.466575 \n",
            "Average reward for training episode 600: 19.37 Test Score: 12.40 Loss: 0.423108 \n",
            "Average reward for training episode 700: 21.37 Test Score: 11.10 Loss: 0.383329 \n",
            "Average reward for training episode 800: 22.59 Test Score: 11.50 Loss: 0.345124 \n",
            "Average reward for training episode 900: 24.39 Test Score: 11.10 Loss: 0.311410 \n",
            "Average reward for training episode 1000: 25.20 Test Score: 15.70 Loss: 0.281942 \n",
            "Average reward for training episode 1100: 27.34 Test Score: 15.30 Loss: 0.256767 \n",
            "Average reward for training episode 1200: 28.17 Test Score: 17.40 Loss: 0.233353 \n",
            "Average reward for training episode 1300: 25.45 Test Score: 55.20 Loss: 0.213734 \n",
            "Average reward for training episode 1400: 27.02 Test Score: 91.20 Loss: 0.196791 \n",
            "Average reward for training episode 1500: 28.37 Test Score: 101.30 Loss: 0.182046 \n",
            "Solved in 1499 episodes!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVfNxrViz5_V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-3qNIaSz7ee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "states = np.empty(0).reshape(0,dimen)\n",
        "actions = np.empty(0).reshape(0,1)\n",
        "rewards = np.empty(0).reshape(0,1)\n",
        "discounted_rewards = np.empty(0).reshape(0,1)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1z1DOtulv19a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2c036bef-4ee8-467c-8b43-f7d58c0820f6"
      },
      "source": [
        "states"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([], shape=(0, 4), dtype=float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFNxRiUXxIyM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "state = np.reshape(observation, [1, dimen])"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUBGIwJgQpPh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "86b2d9ee-0fe4-4218-8787-1080122df5a1"
      },
      "source": [
        "state.shape"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XO0UtNSrQz3Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "states = np.vstack([states, state])"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C26VzC5nQ19v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "990f1580-4e3d-4ec1-ac00-37b2f48b3066"
      },
      "source": [
        "states"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.07202844,  0.97453101, -0.23050895, -1.84188394]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snU79kciRDhH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tes = np.empty(0).reshape(0,2)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vpv8hfHrRPWZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tes1 = np.array([1,2])"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtRY_2oRREli",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tes1= np.reshape(tes1, [1,2])"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EM_ImvY62vDg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tes = np.vstack([tes, tes1])"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZSoJm5f21x6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ea82a70d-6024-4699-d046-73452728d4fb"
      },
      "source": [
        "tes"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 2.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOSyJwY0Rk59",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " predict = model_predict.predict([state])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YyCkEXjRsHP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0b9d76d7-bcee-4da1-9c50-20b7db7c8c7a"
      },
      "source": [
        "predict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.4965743, 0.5034257], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLCjvRtuR-Nf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7a57b5ab-5769-4d05-c106-48d62fd6569b"
      },
      "source": [
        "#num_actions = env.action_space.n\n",
        "action = np.random.choice(range(num_actions),p=predict)\n",
        "print(action)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyZQTDCESG6Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "50c28b7d-4068-47b5-a03c-51314ced476b"
      },
      "source": [
        "(49 + 1) % batch_size == 0"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sFPzMx2SHHw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "84e10341-a9a2-49b1-8cbf-82c4c8275b42"
      },
      "source": [
        "np.random.choice([1,2,3], size=1, replace=True, p=(0.1, 0.5, 0.4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bS_8MxJTU9fC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "states = np.empty(0).reshape(0,dimen)\n",
        "states = np.vstack([states, state])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qk6Qe82zVIIL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eb26abf1-132f-4c25-8851-b6ed9d87900d"
      },
      "source": [
        "states"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.04977838,  0.04408428, -0.01042855,  0.003663  ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    }
  ]
}