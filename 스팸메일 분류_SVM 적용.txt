http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/
이곳에서 SMS spam Collection이라는 파일을 받은후 csv로 변환을 시켜줄 것이다. 
구글 시트를 통해 csv로 변환을 시켜주고 sms_spam.csv라는 이름을 지어주자.



sms_raw <- read.csv(file.choose(), header = TRUE,  stringsAsFactors = FALSE)
str(sms_raw)
sms_raw$type <- factor(sms_raw$type)
library(tm)
sms_corpus <- VCorpus(VectorSource(sms_raw$text))

sms_corpus_clean <- tm_map(sms_corpus, content_transformer(tolower))
sms_corpus_clean <- tm_map(sms_corpus_clean, removeNumbers)
sms_corpus_clean <- tm_map(sms_corpus_clean, removeWords, stopwords())
sms_corpus_clean <- tm_map(sms_corpus_clean, removePunctuation)


library(SnowballC) ## 영어 형태소
sms_corpus_clean <- tm_map(sms_corpus_clean, stemDocument) ## 텍스트 전체에 어근 추출 적용
sms_corpus_clean <- tm_map(sms_corpus_clean, stripWhitespace)
sms_dtm <- DocumentTermMatrix(sms_corpus_clean)
sms_freqwords <- findFreqTerms(sms_dtm, 5) # 최소 5회이상 출현 단어
sms_dtm_freq <- sms_dtm[ , sms_freqwords]

inspect(sms_dtm_freq[1:5,1:3])
sms_m <- as.matrix(sms_dtm_freq)
y <- sms_raw$type
m <- cbind(sms_m, y) 
sms_df <- as.data.frame(m)
sms_df$y <- factor(sms_df$y)

convert_counts <- function(x) {
 x <- ifelse(x > 0, x, 0.000001)
 }

sms_df <- apply(sms_df, MARGIN = 2, convert_counts)
m <- as.matrix(sms_df)
sms_df <- as.data.frame(m)

library(caret)
set.seed(4000)
intrain <- createDataPartition(y=sms_df$y, p=0.7, list=FALSE) # 반환되는 인자가 List이면 FALSE
train <- sms_df[intrain,]
test <- sms_df[-intrain,]

library("e1071")
result <- tune.svm(y~., data=train, gamma=2^(-5:0), cost=2^(0.4), kernel="radial")

## 결과는 

gamma     cost
 0.125 1.319508


sms_svm <- svm(y~., data=train, gamma = 0.125, cost=1, kernel="radial")
sms_svm_predict <- predict(sms_svm, test)
confusionMatrix(sms_svm_predict, test$y)

