{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "policy gradient module",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPFKOJy93UUw9d34Av0wMqS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sjbaek12/sjbaek12.github.io/blob/master/policy_gradient_module.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0u4NOwFyzQBI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.config.run_functions_eagerly\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow import keras\n",
        "import keras.layers as layers\n",
        "from keras.layers import Dense\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam, RMSprop, SGD\n",
        "import keras.backend as K  # 케라스의 backend를 K. 형식으로 호출하는 것이고 여기서는 tensorflow이다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAkjt7CYz4gp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agent(object):\n",
        "  def __init__(self, ALPHA=0.01, GAMMA=0.99, n_actions=4, layer1_size=8, layer2_size=10, input_dims=4, fname='reinforce.h5'):\n",
        "    self.gamma = GAMMA\n",
        "    self.lr = ALPHA\n",
        "    self.G = 0\n",
        "    self.input_dims = input_dims\n",
        "    self.fc1_dims = layer1_size\n",
        "    self.fc2_dims = layer2_size\n",
        "    self.n_actions = n_actions\n",
        "    self.state_memory = []\n",
        "    self.action_memory = []\n",
        "    self.reward_memory = []\n",
        "\n",
        "    self.policy, self.predict = self.build_policy_network()\n",
        "    self.action_space = [i for i in range(n_actions)]\n",
        "    self.model_file = fname\n",
        "\n",
        "  def build_policy_network(self):\n",
        "    input=Input(shape=(self.input_dims,))\n",
        "    advantages = Input(shape=[1])\n",
        "    dense1 = Dense(self.fc1_dims, activation='relu')(input)\n",
        "    dense2 = Dense(self.fc2_dims, activation='relu')(dense1)\n",
        "    probs = Dense(self.n_actions, activation='softmax')(dense2)\n",
        "\n",
        "    def custom_loss(y_true, y_pred):\n",
        "      out = K.clip(y_pred, 1e-8, 1-1e-8)\n",
        "      log_lik = K.sum(y_true*K.log(out), axis=1)*advantages\n",
        "      return -log_lik\n",
        "   \n",
        "    policy = Model(inputs = [input, advantages], outputs = probs) # input = , output= 를 생략한 상태이다.\n",
        "    policy.compile(optimizer = Adam(lr=self.lr), loss=custom_loss)\n",
        "    \n",
        "    predict = Model(inputs = input, outputs = probs) #input = , output= 를 생략한 상태이다.\n",
        "\n",
        "    return policy, predict\n",
        "\n",
        "  def choose_action(self, observation):\n",
        "    state = observation[np.newaxis, :] # [1.0, 1.0]을 [[1.0, 1.0]]와 같이 한축을 추가한다.\n",
        "    probabilities = self.predict.predict(state)[0]\n",
        "    action = np.random.choice(self.action_space, p=probabilities)\n",
        "\n",
        "    return action\n",
        "\n",
        "  def store_transition(self, observation, action, reward):\n",
        "    self.action_memory.append(action)\n",
        "    self.state_memory.append(observation)\n",
        "    self.reward_memory.append(reward)\n",
        "    \n",
        "  def learn(self):\n",
        "    state_memory = np.array(self.state_memory)\n",
        "    action_memory = np.array(self.action_memory)\n",
        "    reward_memory = np.array(self.reward_memory)\n",
        "\n",
        "    y = np.zeros([len(action_memory), self.n_actions])\n",
        "    y[np.arange(len(action_memory)), action_memory] = 1 # action memory 길이만큼 zero 리스트를 만들고, 각 리스트에 선택한 행동에 1을 넣어준다\n",
        "\n",
        "\n",
        "    G= np.zeros_like(reward_memory)\n",
        "    for t in range(len(reward_memory)):\n",
        "      G_sum = 0\n",
        "      discout = 1\n",
        "      for k in range(t, len(reward_memory)):\n",
        "        G_sum += reward_memory[k]*discout\n",
        "        discout *= self.gamma\n",
        "\n",
        "      G[t] = G_sum\n",
        "    mean = np.mean(G)\n",
        "    std = np.std(G) if np.std(G) > 0 else 1\n",
        "    self.G = (G-mean)/std\n",
        "\n",
        "    self.G = np.reshape(self.G, (10,1))\n",
        "\n",
        "\n",
        "    history = self.policy.train_on_batch([state_memory, self.G], y)\n",
        "\n",
        " \n",
        "\n",
        "    self.state_memory= []\n",
        "    self.action_memory = []\n",
        "    self.reward_memory = []\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls9VSaPI6yjW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "c4d5ecd3-6d21-497e-ab31-b446f946eaf0"
      },
      "source": [
        "agent = Agent(ALPHA=0.01, GAMMA=0.99, n_actions=4, layer1_size=8, layer2_size=10, input_dims=4, fname='reinforce.h5')\n",
        "\n",
        "score_history = []\n",
        "\n",
        "n_episodes = 4000\n",
        "\n",
        "bandits = [20,10,5,-10]\n",
        "\n",
        "def pullBandit(chosen_action):\n",
        "  result = np.random.randn(1)\n",
        "  if result > bandits[chosen_action]:\n",
        "    reward = 1.0\n",
        "    return reward\n",
        "  else:\n",
        "    reward = 0.0\n",
        "    return reward\n",
        "\n",
        "weights = np.array([1.0, 1.0, 1.0, 1.0])\n",
        "\n",
        "for i in range(n_episodes):\n",
        "  done = 0\n",
        "  score = 0\n",
        "  observation = weights\n",
        "\n",
        "  while done < 10:\n",
        "    action = agent.choose_action(observation)\n",
        "    reward = pullBandit(action)\n",
        "    agent.store_transition(observation, action, reward)\n",
        "    score += reward\n",
        "    done += 1\n",
        "  score_history.append(score)\n",
        "\n",
        "  agent.learn()\n",
        "\n",
        "#  print('episode', i, 'score:', score_history[i])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py:3350: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n",
            "  \"Even though the tf.config.experimental_run_functions_eagerly \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMwzgxHyapfF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPhOa6G8hwRZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "zero = 0\n",
        "one = 0\n",
        "two = 0\n",
        "three = 0\n",
        "four = 0\n",
        "five = 0\n",
        "six = 0\n",
        "seven = 0\n",
        "\n",
        "for i in range(len(score_history)):\n",
        "  if i < 3000:\n",
        "    continue\n",
        "  if score_history[i] == 0:\n",
        "    zero += 1\n",
        "  if score_history[i] == 1:\n",
        "    one += 1\n",
        "  if score_history[i] == 2:\n",
        "    two += 1\n",
        "  if score_history[i] == 3:\n",
        "    three += 1\n",
        "  if score_history[i] == 4:\n",
        "    four += 1\n",
        "  if score_history[i] == 5:\n",
        "    five += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UP13pb1ejnpb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f725bf81-4230-4553-f68a-5ba7d44c0ede"
      },
      "source": [
        "print(one, two, three, four, five)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "312 288 186 74 16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s539cwc29msu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "412fd71c-5bd4-4daa-b88d-68360e66807f"
      },
      "source": [
        "w = np.array(weights)\n",
        "w[np.newaxis, :]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 1., 1., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    }
  ]
}