{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bandit policy gradient",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPA0K0uzk4dlUJzgUoarK7k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sjbaek12/sjbaek12.github.io/blob/master/bandit_policy_gradient.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Arxle_f33H9J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AidNzqhWCa8M",
        "colab_type": "text"
      },
      "source": [
        "높은 보상을 일으키는 행동의 weights를 증가시킨다.\n",
        "\n",
        "이것은 보상에 대한 그라디언트가 아니라 Policy 함수를 직접 편미분하는 방식이다. 아래 예제는 정확한 답을 구해내고는 있지만 사실 이것은 policy gradient가 아니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQMy8s6tChpy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bandits = [0.2,0,-0.2,-5]\n",
        "\n",
        "def pullBandit(chosen_action):\n",
        "  result = np.random.randn(1)\n",
        "  rewards = [0.0, 0.0, 0.0, 0.0]\n",
        "  if result > bandits[chosen_action]:\n",
        "    rewards[chosen_action] = 1.0\n",
        "    return rewards\n",
        "  else:\n",
        "    rewards[chosen_action] = -1.0\n",
        "    return rewards\n",
        "\n",
        "num_episode = 4000\n",
        "\n",
        "episode = 0 \n",
        "\n",
        "e = 0.5\n",
        "\n",
        "weights = [0.1, 0.1, 0.1, 0.1]\n",
        "\n",
        "small = 0.000000000001\n",
        "\n",
        "def deriva():       # policy 함수를 구성하는 웨이트의 그래디언트를 직접 구하는 함수이다\n",
        "  dv = []\n",
        "  for i in range(len(weights)):\n",
        "    prob_chosen_small = (weights[i]+small)/(np.sum(weights)+small)\n",
        "    prob_chosen = (weights[i])/np.sum(weights)\n",
        "    d = (prob_chosen_small - prob_chosen)/small\n",
        "    dv.append(d)\n",
        "  return dv\n",
        "\n",
        "\n",
        "while episode < num_episode:\n",
        "  \n",
        "  if np.random.rand(1) < e:\n",
        "    chosen_action = np.random.randint(4)    \n",
        "  else:\n",
        "    probs = [weights[0]/np.sum(weights), weights[1]/np.sum(weights), weights[2]/np.sum(weights), weights[3]/np.sum(weights)]\n",
        "    chosen_action = np.argmax(probs)\n",
        "    \n",
        "  rewards = pullBandit(chosen_action)\n",
        "  dv = deriva()\n",
        "  weights[chosen_action] = max((weights[chosen_action] + 0.01*np.matmul(dv,rewards), 0))\n",
        "  \n",
        "  \n",
        "  if episode % 100 == 0:\n",
        "    print(chosen_action, rewards, weights)\n",
        "\n",
        "  e = e * (1- episode/num_episode)\n",
        "\n",
        "\n",
        "  episode = episode + 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtNWEE7xvRsX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGduvz9Umi8J",
        "colab_type": "text"
      },
      "source": [
        "policy gradient는 목적함수인 E[r]을 최대화 하는 것이다.\n",
        "\n",
        "\n",
        "L = − Q(s,a)logπ(a|s)\n",
        "\n",
        "Q(s,a)는 Accumulated total reward 이다. 즉 특정 시점에서 행동 a를 했을때, 그 이후 게임이 끝날때까지 받을 수 있는 reward의 합을 말한다.\n",
        "\n",
        "Q(s,a)를 사전에 알 수 없기 때문에 Policy Gradient에서는 게임을 끝까지 해보고, \n",
        "그 결과로 나온 reward를 바탕으로 Q(s,a)를 근사적으로 구하는데 이들을 normalize해서 사용한다. 그 이유는 샘플을 사용해서 Q(s,a)를 구하는 것이므로 이들의 평균치를 사용하는 것이다.\n",
        "\n",
        "\n",
        "logπ(a|s)는 특정시점에서 상태 s에서 a 행동을 선택할 확률의 로그값이다. 이는 cross-binary loss를 사용할 경우  L = - log(a)*y에서 y=1이면 L = - log(a)와 동일하다. 왜냐하면 a는 y를 선택할 확률값(softmax)와 동일하기 때문이다.\n",
        "\n",
        "따라서 Loss 함수에 Q(s,a)를 곱해주면 되는 것이다.\n",
        "\n",
        "각각의 예를 생각해보면,\n",
        "\n",
        "Q(s,a)가 클때 a가 작다면 리턴은 크나 그 것이 선택될 확률이 작다는 것을 의미하고 이경우에는 L값이 커지므로 gradient를 변동시킬 것이다. \n",
        "\n",
        "Q(s,a)가 클때 a가 크다면 리턴은 크나 그 것이 선택될 확률이 크다는 것을 의미하고 이경우에는 L값이 0에 가까워지므로 gradient를 변동시키지 않을 것이다.\n",
        "\n",
        "Q(s,a)가 작으나 a가 크다면 0에 가까와 지므로 gradient를 변동시키지 않을 것이다.\n",
        "\n",
        "Q(s,a)가 작고 a도 작다면 L이 크므로 gradient를 변동시킬 것이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gySRedrBmDoD",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoZXN8eqvjIk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "import keras.layers as layers\n",
        "from keras.layers import Dense\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam, RMSprop, SGD\n",
        "import keras.backend as K  # 케라스의 backend를 K. 형식으로 호출하는 것이고 여기서는 tensorflow이다.\n",
        "import tensorflow as tf"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyt1V4DJwLR1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def custom_loss_test1(y_true, y_pred):\n",
        "  log_lik = K.sum(K.log(y_pred + 1e-20)*y_true, axis=1)*adv\n",
        "  return -log_lik # 모든 샘플의 크로스엔트로피의 평균인데, n의 배수로 나누어준 값이 된다.\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z--c2DjYvknk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_policy_model(lr):\n",
        "  \n",
        "  adv = layers.Input(shape=[1])\n",
        "  inp_L = layers.Input(shape=(4,)) \n",
        "  dense1 = Dense(8,activation=\"relu\")(inp_L) \n",
        "  dense1 = Dense(10, activation=\"relu\")(dense1)\n",
        "  output_s = layers.Dense(4, activation=\"softmax\")(dense1)\n",
        "\n",
        "  model_train = Model(inputs=[inp_L, adv], outputs = output_s)\n",
        "#  model_train = Model(inputs=inp_L, outputs = output_s)\n",
        "  model_train.compile(loss=custom_loss_test1, optimizer=SGD(lr), metrics = ['mae'])\n",
        "  model_predict = Model(inputs=inp_L, outputs=output_s)   # model_train.predict와 model_predict.predict의 결과는 완전히 일치한다. 다만 입력의 편의를 위해서 분리한다.\n",
        "  return model_train, model_predict"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fg2E1jlywwql",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_train, model_predict = get_policy_model(0.1)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7mJoEmswzlc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "dae31229-308d-47de-b12d-232cd59eb527"
      },
      "source": [
        "model_train.summary()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_13\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_8 (InputLayer)            [(None, 4)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 8)            40          input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 10)           90          dense_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_7 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 4)            44          dense_10[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 174\n",
            "Trainable params: 174\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWobhJtvw1sU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "18a821ab-c47f-46d0-b467-1b97956ed96e"
      },
      "source": [
        "bandits = [20,10,5,-10]\n",
        "\n",
        "def pullBandit(chosen_action):\n",
        "  result = np.random.randn(1)\n",
        "  rewards = [0.0, 0.0, 0.0, 0.0]\n",
        "  if result > bandits[chosen_action]:\n",
        "    rewards[chosen_action] = 1.0\n",
        "    return rewards\n",
        "  else:\n",
        "    rewards[chosen_action] = 0.0\n",
        "    return rewards\n",
        "\n",
        "num_episode = 4000\n",
        "\n",
        "episode = 0 \n",
        "\n",
        "\n",
        "weights = [1.0, 1.0, 1.0, 1.0]\n",
        "score = [0.0, 0.0, 0.0, 0.0]\n",
        "\n",
        "reward_memory = []\n",
        "action_memory =[]\n",
        "\n",
        "while episode < num_episode:\n",
        "  \n",
        "  for i in range(10):\n",
        "    \n",
        "    probs = model_predict.predict([weights])\n",
        "    chosen_action = np.random.choice(4, p=probs[0])\n",
        "    \n",
        "    rewards = np.sum(pullBandit(chosen_action))\n",
        "    reward_memory.append(rewards)\n",
        "\n",
        "    b = [0.0, 0.0, 0.0,0.0]\n",
        "    b[chosen_action] = 1.0\n",
        "    action_memory.append(b)\n",
        "  \n",
        "  y = np.array(action_memory) \n",
        "\n",
        "  G = np.zeros_like(reward_memory)\n",
        "\n",
        "  for t in range(len(reward_memory)):\n",
        "    G_sum = 0.0\n",
        "    discount = 1\n",
        "    gamma = 1\n",
        "\n",
        "    for k in range(t, len(reward_memory)):\n",
        "      G_sum += reward_memory[k]*discount\n",
        "      discount *= gamma\n",
        "\n",
        "    G[t]= G_sum\n",
        "\n",
        "  mean = np.mean(G)\n",
        "  std = np.std(G) if np.std(G) > 0 else 1\n",
        "  G = (G-mean)/std \n",
        "\n",
        "  G = np.array(G)\n",
        "  adv = np.reshape(G, (10,1))\n",
        "\n",
        "  \n",
        "#  adv = np.array([[np.sum(pullBandit(chosen_action))]])\n",
        "  \n",
        "  a = []\n",
        "  for i in range(len(reward_memory)):\n",
        "    a.append(weights)\n",
        "  a = np.array(a)\n",
        "\n",
        "  \n",
        "  loss=model_train.train_on_batch([a, adv], y)\n",
        "\n",
        "  reward_memory = []\n",
        "  action_memory =[]\n",
        "\n",
        "  if episode % 500 == 0:\n",
        "    print(episode, chosen_action, \"p_act1:{:0.2f}, p_act2:{:0.2f}, p_act3:{:0.2f}, p_act4:{:0.2f}  \".format(probs[0][0],probs[0][1],probs[0][2],probs[0][3]))\n",
        "\n",
        "  episode = episode + 1\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 3 p_act1:0.19, p_act2:0.21, p_act3:0.29, p_act4:0.31  \n",
            "500 1 p_act1:0.19, p_act2:0.21, p_act3:0.29, p_act4:0.31  \n",
            "1000 3 p_act1:0.19, p_act2:0.21, p_act3:0.29, p_act4:0.31  \n",
            "1500 2 p_act1:0.19, p_act2:0.21, p_act3:0.29, p_act4:0.31  \n",
            "2000 0 p_act1:0.19, p_act2:0.21, p_act3:0.29, p_act4:0.31  \n",
            "2500 0 p_act1:0.19, p_act2:0.21, p_act3:0.29, p_act4:0.31  \n",
            "3000 0 p_act1:0.19, p_act2:0.21, p_act3:0.29, p_act4:0.31  \n",
            "3500 0 p_act1:0.19, p_act2:0.21, p_act3:0.29, p_act4:0.31  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6O8rlveGIWw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "44bacd38-32b8-4839-d09e-2fe84274b3d4"
      },
      "source": [
        "G"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.2456822,  1.2456822,  0.4152274,  0.4152274,  0.4152274,\n",
              "        0.4152274, -0.4152274, -0.4152274, -1.2456822, -2.076137 ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WTqj2Ae9Kxr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "76e1bcb8-f5af-4a13-cbb5-f6adb2c6e3bf"
      },
      "source": [
        "G+mean/std"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3.32181919, 3.32181919, 2.4913644 , 2.4913644 , 2.4913644 ,\n",
              "       2.4913644 , 1.6609096 , 1.6609096 , 0.8304548 , 0.        ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgBjoOFEZTm8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8cff769f-091a-4f21-d791-00d082604d89"
      },
      "source": [
        "print(\"averge: {:0.2f}\".format(0.009001))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "averge: 0.01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3uZfskoyhAn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a7dfac22-94be-4ca2-cbae-d17a90b7df1e"
      },
      "source": [
        " model_predict.predict([weights])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.19022828, 0.20891608, 0.29173118, 0.3091244 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxACHI6-F4qW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  a = []\n",
        "  for i in range(len(reward_memory)):\n",
        "    a.append(weights)\n",
        "  a = np.array(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9KS5rX5voH6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8f720c77-6eb1-4d34-cc71-85a040715ebf"
      },
      "source": [
        "a = [0.0, 0.0, 0.0, 0.0]\n",
        "b = [1.0, 1.0, 1.0, 1.0]\n",
        "\n",
        "[sum(x) for x in zip(a, b)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0, 1.0, 1.0, 1.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZDmmtkf47FN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "21df0b4c-b004-47df-b315-d6f930738978"
      },
      "source": [
        "def custom_loss_test2(y_true, y_pred):\n",
        "  log_lik = K.sum(K.log(y_true * (y_true - y_pred) + (1-y_true) * (y_true+y_pred)))*adv\n",
        "#\n",
        "  return log_lik # 모든 샘플의 크로스엔트로피의 평균인데, n의 배수로 나누어준 값이 된다.\n",
        "\n",
        "\n",
        "y_pred = np.array([[0.99, 0.5, 0.1, 0.1]])\n",
        "y_true = np.array([[1.0,0.0,0.0, 0.0]])\n",
        "\n",
        "adv = np.array([1.0]) \n",
        "\n",
        "custom_loss_test2(y_true, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1,), dtype=float64, numpy=array([-9.90348755])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiwVv46_KS5l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "29455952-1035-461c-b795-7570bbcb799d"
      },
      "source": [
        "a = [0.1, 0.1, 0.1, 0.1]\n",
        "\n",
        "a = np.array([a])\n",
        "\n",
        " \n",
        "\n",
        "b = np.array([[0.0, 1.0, 0.0, 0.0]])\n",
        "\n",
        "adv = np.array([[-1.0]])\n",
        "\n",
        "history = model_train.train_on_batch([a, adv],b)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 17 calls to <function Model.make_train_function.<locals>.train_function at 0x7f58c8447730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZIw3rSDhEFQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "883200ed-1a54-41eb-c197-5c2c206351bd"
      },
      "source": [
        "class Agendt(object):\n",
        "  def __init__(self, ALPHA, )\n",
        "\n",
        "\n",
        "y_pred = np.array([[0.9, 0.9, 0.1, 0.0]])\n",
        "y_true = np.array([[1.0,0.0,0.0, 0.0]])\n",
        "\n",
        "adv = np.array([1.0]) \n",
        "\n",
        "print(custom_loss(y_true, y_pred))\n",
        "\n",
        "def choose_action(observation):\n",
        "  probabilites = model_predict.predict()\n",
        "  action = np.random.choice(4, p=probabilities)\n",
        "  return action\n",
        "\n",
        "def store_transtion(observation, action, reward):\n",
        "  action_memory.append"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(0.10536051565782628, shape=(), dtype=float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxnpweoojnqG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def custom_loss(y_true, y_pred):\n",
        "  out = K.clip(y_pred, 1e-8, 1-1e-8)\n",
        "  log_lik = y_true*K.log(out)*adv\n",
        "\n",
        "  return K.sum(-log_lik)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}